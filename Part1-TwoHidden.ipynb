{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning with PyTorch and Scikit-Learn  \n",
    "# -- Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and preparing the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to [-1, 1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ((X / 255.) - .5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "#     X, y, test_size=10000, random_state=123, stratify=y)\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.7,test_size=0.3)\n",
    "\n",
    "# optional to free up some memory by deleting non-used arrays:\n",
    "# del X_temp, y_temp, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(X - np.max(X))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "\n",
    "class NeuralNetMLP2:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes, random_seed=123, softmaxUse=False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_hidden1 = num_hidden\n",
    "        num_hidden2 = num_hidden\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # hidden first\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "\n",
    "        self.weight_h = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden1, num_features))\n",
    "        self.bias_h = np.zeros(num_hidden1)\n",
    "\n",
    "        # hidden second\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "\n",
    "        self.weight_h2 = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden2, num_hidden1))\n",
    "        self.bias_h2 = np.zeros(num_hidden2)\n",
    "\n",
    "        # output\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_classes, num_hidden2))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "        self.softmaxUse = softmaxUse\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        z_h1 = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        a_h1 = sigmoid(z_h1)\n",
    "\n",
    "        # Hidden layer 2\n",
    "        z_h2 = np.dot(a_h1, self.weight_h2.T) + self.bias_h2\n",
    "        a_h2 = sigmoid(z_h2)\n",
    "\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden] dot [n_classes, n_hidden].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h2, self.weight_out.T) + self.bias_out\n",
    "        a_out = sigmoid(z_out)\n",
    "\n",
    "        return a_h1, a_h2, a_out\n",
    "\n",
    "    def backward(self, x, a_h1, a_h2, a_out, y):\n",
    "\n",
    "        #########################\n",
    "        # Output layer weights\n",
    "        #########################\n",
    "\n",
    "        # onehot encoding\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "\n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        # = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        # where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        # for convenient re-use\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = a_out * (1. - a_out)  # sigmoid derivative\n",
    "\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        # \"delta (rule) placeholder\"\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out\n",
    "\n",
    "        # gradient for output weights\n",
    "\n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_out__dw_out = a_h2\n",
    "\n",
    "        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]\n",
    "        # output dim: [n_classes, n_hidden]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "\n",
    "        #################################\n",
    "        # Part 2: dLoss/dHiddenWeights\n",
    "        # = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "\n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_out\n",
    "\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "\n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h2 * (1. - a_h2)  # sigmoid derivative\n",
    "\n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = a_h1\n",
    "\n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h2 = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h2 = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "    # ME ADD\n",
    "        delta_out = d_loss__a_h * d_a_h__d_z_h\n",
    "\n",
    "        #################################\n",
    "        # Part 3: dLoss/dHiddenWeights\n",
    "        # = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "\n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_h2\n",
    "\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "\n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h1 * (1. - a_h1)  # sigmoid derivative\n",
    "\n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "\n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h1 = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h1 = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out,\n",
    "                d_loss__d_w_h1, d_loss__d_b_h1,\n",
    "                d_loss__d_w_h2, d_loss__d_b_h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetMLP2(num_features=28*28,\n",
    "                     num_hidden=500,\n",
    "                     num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the neural network training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "        \n",
    "# iterate over training epochs\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # iterate over minibatches\n",
    "    minibatch_gen = minibatch_generator(\n",
    "        X_train, y_train, minibatch_size)\n",
    "    \n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "        \n",
    "    break\n",
    "    \n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to compute the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets)\n",
    "\n",
    "\n",
    "def predict(model, X):\n",
    "    _, _, probas = model.forward(X)\n",
    "    predicted_labels = np.argmax(probas, axis=1)\n",
    "    return predicted_labels\n",
    "\n",
    "# _, _, probas = model.forward(X_valid)\n",
    "# mse = mse_loss(y_valid, probas)\n",
    "\n",
    "# predicted_labels = np.argmax(probas, axis=1)\n",
    "# acc = accuracy(y_valid, predicted_labels)\n",
    "\n",
    "# print(f'Initial validation MSE: {mse:.1f}')\n",
    "# print(f'Initial validation accuracy: {acc*100:.1f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _,_, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/i\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial valid MSE: 0.4\n",
      "Initial valid accuracy: 11.5%\n"
     ]
    }
   ],
   "source": [
    "mse, acc = compute_mse_and_acc(model, X_test, y_test)\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # iterate over minibatches\n",
    "        minibatch_gen = minibatch_generator(\n",
    "            X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "            #### Compute outputs ####\n",
    "            a_h1, a_h2, a_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Compute gradients ####\n",
    "            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h1, d_loss__d_b_h1, d_loss__d_w_h2, d_loss__d_b_h2 = \\\n",
    "                model.backward(X_train_mini, a_h1, a_h2, a_out, y_train_mini)\n",
    "\n",
    "            #### Update weights ####\n",
    "            model.weight_h -= learning_rate * d_loss__d_w_h1\n",
    "            model.bias_h -= learning_rate * d_loss__d_b_h1\n",
    "\n",
    "            model.weight_h2 -= learning_rate * d_loss__d_w_h2\n",
    "            model.bias_h2 -= learning_rate * d_loss__d_b_h2\n",
    "            \n",
    "            model.weight_out -= learning_rate * d_loss__d_w_out\n",
    "            model.bias_out -= learning_rate * d_loss__d_b_out\n",
    "\n",
    "        #### Epoch Logging ####\n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Train MSE: 0.03 | Train Acc: 84.39% | Valid Acc: 84.45%\n",
      "Epoch: 002/050 | Train MSE: 0.02 | Train Acc: 87.72% | Valid Acc: 87.79%\n",
      "Epoch: 003/050 | Train MSE: 0.02 | Train Acc: 89.04% | Valid Acc: 89.12%\n",
      "Epoch: 004/050 | Train MSE: 0.02 | Train Acc: 89.93% | Valid Acc: 89.90%\n",
      "Epoch: 005/050 | Train MSE: 0.02 | Train Acc: 90.67% | Valid Acc: 90.67%\n",
      "Epoch: 006/050 | Train MSE: 0.02 | Train Acc: 91.08% | Valid Acc: 91.10%\n",
      "Epoch: 007/050 | Train MSE: 0.02 | Train Acc: 91.34% | Valid Acc: 91.36%\n",
      "Epoch: 008/050 | Train MSE: 0.01 | Train Acc: 91.95% | Valid Acc: 91.79%\n",
      "Epoch: 009/050 | Train MSE: 0.01 | Train Acc: 92.20% | Valid Acc: 92.06%\n",
      "Epoch: 010/050 | Train MSE: 0.01 | Train Acc: 92.55% | Valid Acc: 92.44%\n",
      "Epoch: 011/050 | Train MSE: 0.01 | Train Acc: 92.80% | Valid Acc: 92.51%\n",
      "Epoch: 012/050 | Train MSE: 0.01 | Train Acc: 92.97% | Valid Acc: 92.88%\n",
      "Epoch: 013/050 | Train MSE: 0.01 | Train Acc: 93.45% | Valid Acc: 93.15%\n",
      "Epoch: 014/050 | Train MSE: 0.01 | Train Acc: 93.64% | Valid Acc: 93.38%\n",
      "Epoch: 015/050 | Train MSE: 0.01 | Train Acc: 93.83% | Valid Acc: 93.44%\n",
      "Epoch: 016/050 | Train MSE: 0.01 | Train Acc: 94.09% | Valid Acc: 93.67%\n",
      "Epoch: 017/050 | Train MSE: 0.01 | Train Acc: 94.24% | Valid Acc: 93.89%\n",
      "Epoch: 018/050 | Train MSE: 0.01 | Train Acc: 94.35% | Valid Acc: 93.77%\n",
      "Epoch: 019/050 | Train MSE: 0.01 | Train Acc: 94.61% | Valid Acc: 94.10%\n",
      "Epoch: 020/050 | Train MSE: 0.01 | Train Acc: 94.72% | Valid Acc: 94.18%\n",
      "Epoch: 021/050 | Train MSE: 0.01 | Train Acc: 94.87% | Valid Acc: 94.37%\n",
      "Epoch: 022/050 | Train MSE: 0.01 | Train Acc: 94.91% | Valid Acc: 94.41%\n",
      "Epoch: 023/050 | Train MSE: 0.01 | Train Acc: 95.06% | Valid Acc: 94.52%\n",
      "Epoch: 024/050 | Train MSE: 0.01 | Train Acc: 95.16% | Valid Acc: 94.76%\n",
      "Epoch: 025/050 | Train MSE: 0.01 | Train Acc: 95.25% | Valid Acc: 94.76%\n",
      "Epoch: 026/050 | Train MSE: 0.01 | Train Acc: 95.45% | Valid Acc: 94.91%\n",
      "Epoch: 027/050 | Train MSE: 0.01 | Train Acc: 95.61% | Valid Acc: 95.05%\n",
      "Epoch: 028/050 | Train MSE: 0.01 | Train Acc: 95.63% | Valid Acc: 95.06%\n",
      "Epoch: 029/050 | Train MSE: 0.01 | Train Acc: 95.75% | Valid Acc: 95.13%\n",
      "Epoch: 030/050 | Train MSE: 0.01 | Train Acc: 95.81% | Valid Acc: 95.22%\n",
      "Epoch: 031/050 | Train MSE: 0.01 | Train Acc: 96.01% | Valid Acc: 95.41%\n",
      "Epoch: 032/050 | Train MSE: 0.01 | Train Acc: 96.08% | Valid Acc: 95.45%\n",
      "Epoch: 033/050 | Train MSE: 0.01 | Train Acc: 96.20% | Valid Acc: 95.53%\n",
      "Epoch: 034/050 | Train MSE: 0.01 | Train Acc: 96.29% | Valid Acc: 95.53%\n",
      "Epoch: 035/050 | Train MSE: 0.01 | Train Acc: 96.38% | Valid Acc: 95.71%\n",
      "Epoch: 036/050 | Train MSE: 0.01 | Train Acc: 96.39% | Valid Acc: 95.73%\n",
      "Epoch: 037/050 | Train MSE: 0.01 | Train Acc: 96.55% | Valid Acc: 95.80%\n",
      "Epoch: 038/050 | Train MSE: 0.01 | Train Acc: 96.58% | Valid Acc: 95.85%\n",
      "Epoch: 039/050 | Train MSE: 0.01 | Train Acc: 96.66% | Valid Acc: 95.89%\n",
      "Epoch: 040/050 | Train MSE: 0.01 | Train Acc: 96.72% | Valid Acc: 95.98%\n",
      "Epoch: 041/050 | Train MSE: 0.01 | Train Acc: 96.82% | Valid Acc: 96.05%\n",
      "Epoch: 042/050 | Train MSE: 0.01 | Train Acc: 96.87% | Valid Acc: 96.04%\n",
      "Epoch: 043/050 | Train MSE: 0.01 | Train Acc: 96.94% | Valid Acc: 96.13%\n",
      "Epoch: 044/050 | Train MSE: 0.01 | Train Acc: 96.94% | Valid Acc: 96.16%\n",
      "Epoch: 045/050 | Train MSE: 0.01 | Train Acc: 97.03% | Valid Acc: 96.19%\n",
      "Epoch: 046/050 | Train MSE: 0.01 | Train Acc: 97.10% | Valid Acc: 96.24%\n",
      "Epoch: 047/050 | Train MSE: 0.01 | Train Acc: 97.19% | Valid Acc: 96.35%\n",
      "Epoch: 048/050 | Train MSE: 0.01 | Train Acc: 97.22% | Valid Acc: 96.39%\n",
      "Epoch: 049/050 | Train MSE: 0.01 | Train Acc: 97.22% | Valid Acc: 96.31%\n",
      "Epoch: 050/050 | Train MSE: 0.01 | Train Acc: 97.33% | Valid Acc: 96.50%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123) # for the training set shuffling\n",
    "\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc = train(\n",
    "    model, X_train, y_train, X_test, y_test,\n",
    "    num_epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.50%\n"
     ]
    }
   ],
   "source": [
    "test_mse, test_acc = compute_mse_and_acc(model, X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "91257b870c480b7dd5c19d08a02ed6ad036ba039e4ab87aa2676ecb44f07d1cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
